{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd03dbf2-7e12-4b6d-b325-2c4f10ea2ab3",
   "metadata": {},
   "source": [
    "# RAG Using Different LLM from RAG Essentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5177eb7-aa76-4a82-80e6-d6b195f7da53",
   "metadata": {},
   "source": [
    "### This notebook uses PCAI RAG ESSENTIALS End-Points to Build a RAG Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3722c7fb-60c9-4eeb-89de-6d3e8750235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat my-private-ca.crt >> $(python -m certifi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc737e3-dc25-4744-9295-7fcd26c2de29",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e24953-16c6-4ce5-8ff4-8462ce84a602",
   "metadata": {},
   "source": [
    "\n",
    "- **`ChatNVIDIA` & `NVIDIAEmbeddings`**  \n",
    "  From `langchain_nvidia_ai_endpoints`: These provide access to NVIDIA's LLM and embedding models for tasks like chat and semantic search.\n",
    "\n",
    "- **`NVIDIARerank`**  \n",
    "  A reranker module from NVIDIA to reorder retrieved documents based on relevance scores, improving final answer quality.\n",
    "\n",
    "- **`WeaviateVectorStore`**  \n",
    "  An integration with Weaviate, a vector database used to store and query embedding vectors for semantic search.\n",
    "\n",
    "- **`RetrievalQA`**  \n",
    "  A LangChain chain that combines a retriever and a language model to answer questions based on retrieved documents.\n",
    "\n",
    "- **`PyPDFLoader`**  \n",
    "  A document loader that reads and extracts text content from PDF files.\n",
    "\n",
    "- **`ContextualCompressionRetriever`**  \n",
    "  A retriever wrapper that compresses context intelligently before passing it to the LLM, often using re-ranking or summarization.\n",
    "\n",
    "- **`CharacterTextSplitter`**  \n",
    "  A utility to split large documents into smaller, overlapping text chunks based on character count (useful for chunking PDFs for RAG).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0197765d-6014-4434-bb59-daf1ea4534cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langchain_nvidia_ai_endpoints.reranking import NVIDIARerank\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d529650-d181-4dff-a9b5-e2bc2ef62bad",
   "metadata": {},
   "source": [
    "## Fetching the Secret Token for RAG Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a304d1-baba-4586-bd23-bddf732c3682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate, os\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "#getting the auth token\n",
    "secret_file_path = \"/etc/secrets/ezua/.auth_token\"\n",
    "\n",
    "with open(secret_file_path, \"r\") as file:\n",
    "    token = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1338716-31b8-420e-ade5-246d64632828",
   "metadata": {},
   "outputs": [],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb97a8d-2021-4ef5-86fd-66f611bdedf1",
   "metadata": {},
   "source": [
    "## Connecting to Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263a85b7-022f-43f6-a730-42f73e969422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/.well-known/openid-configuration \"HTTP/1.1 404 Not Found\"\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/meta \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/.well-known/ready \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "domain = \".cluster.local\"\n",
    "http_host = \"weaviate.hpe-weaviate.svc.cluster.local\"\n",
    "grpc_host = \"weaviate-grpc.hpe-weaviate.svc\" + domain\n",
    "weaviate_headers = {\"x-auth-token\": token}\n",
    "#weaviate_headers = {\"x-auth-token\": \"wrong token\"}\n",
    "\n",
    "client = weaviate.connect_to_custom(\n",
    "    http_host=http_host,        # Hostname for the HTTP API connection\n",
    "    http_port=80,              # Default is 80, WCD uses 443\n",
    "    http_secure=False,           # Whether to use https (secure) for the HTTP API connection\n",
    "    grpc_host=grpc_host,        # Hostname for the gRPC API connection\n",
    "    grpc_port=50051,              # Default is 50051, WCD uses 443\n",
    "    grpc_secure=False,           # Whether to use a secure channel for the gRPC API connection\n",
    "    headers=weaviate_headers,\n",
    "    skip_init_checks=False\n",
    ")\n",
    "\n",
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b74898d-a73f-4ede-831c-6832fb995102",
   "metadata": {},
   "source": [
    "## Connecting to LLM Through RAG ESSENTIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2754a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_endpoint_rag_essentials = \"https://llama-3-1-8b-b7ee1686-predictor-ezai-services.pcai1.genai1.hou\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375b13c-4a19-44ba-8c2e-bafef00bff41",
   "metadata": {},
   "source": [
    "`base_url=llm_endpoint_rag_essentials`\n",
    "The endpoint URL for accessing the NVIDIA-hosted LLM. This is where the requests will be sent.\n",
    "\n",
    "`model=\"meta/llama-3.1-8b-instruct`\n",
    "Specifies the model to use. In this case, it's Metaâ€™s LLaMA 3.1 model with 8 billion parameters, optimized for instruction-following tasks.\n",
    "\n",
    "`api_key=token`\n",
    "Your API key used to authenticate the requests. This should be kept secure and typically stored in an environment variable.\n",
    "\n",
    "`temperature=0.5`\n",
    "Controls randomness in generation.\n",
    "\n",
    "Lower values like 0.5 make the output more focused and deterministic.\n",
    "\n",
    "Higher values increase creativity but reduce reliability.\n",
    "\n",
    "`max_tokens=1024`\n",
    "Maximum number of tokens (words or word-parts) the model can generate in the response.\n",
    "\n",
    "`top_p=1.0`\n",
    "Implements nucleus (top-p) sampling. The model considers only the smallest set of tokens whose cumulative probability is at least p.\n",
    "\n",
    "Setting it to 1.0 disables nucleus filtering, allowing full probability distribution sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5753ad95-48af-4376-81e5-d5d0ec855a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:154: UserWarning: https://llama-3-1-8b-b7ee1686-predictor-ezai-services.pcai1.genai1.hou does not end in /v1, you may have inference and listing issues. This check will be deprecated in the next release. Please ensure /v1 is appended to the provided URL\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatNVIDIA(\n",
    "    base_url=llm_endpoint_rag_essentials,\n",
    "    model=\"meta/llama-3.1-8b-instruct\",\n",
    "    api_key=token,\n",
    "    temperature=0.5,\n",
    "    max_tokens=1024,\n",
    "    top_p=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43d1851-c89b-4a96-908d-73978a46498a",
   "metadata": {},
   "source": [
    "## Data Extraction and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4d614-ea8e-49b9-b52d-c3b895dae718",
   "metadata": {},
   "source": [
    "`pdf_path = \"./HPE.pdf\"`\n",
    "Specifies the path to the PDF file that needs to be processed.\n",
    "\n",
    "`loader = PyPDFLoader(pdf_path)`\n",
    "Creates a PyPDFLoader instance to handle reading and parsing of the PDF file.\n",
    "\n",
    "`documents = loader.load()`\n",
    "Loads the content of the PDF into a list of Document objects, where each object typically represents one page.\n",
    "\n",
    "CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "Initializes a text splitter that breaks down long documents into smaller chunks:\n",
    "\n",
    "Each chunk will be up to 500 characters.\n",
    "\n",
    "There will be a 50-character overlap between chunks to preserve context.\n",
    "\n",
    "`docs = text_splitter.split_documents(documents)`\n",
    "Applies the splitter to the list of loaded documents to produce a list of text chunks.\n",
    "\n",
    "`for doc in docs: doc.metadata = {}`\n",
    "Clears any metadata associated with the text chunks (e.g., page number, source), which can be useful for privacy or minimizing storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74087bad-8c82-4c54-bbd3-033e77ffaf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the path to your PDF\n",
    "pdf_path = \"./HPE.pdf\"\n",
    "\n",
    "# Load PDF file\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into manageable chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "for doc in docs:\n",
    "    doc.metadata={}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b8b83e-3e1b-48c2-99e3-1ba7d4fab411",
   "metadata": {},
   "source": [
    "## Vector Store Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2529bae0-4060-4120-b248-6276f42aae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fadceb4-b74b-48b0-9a01-34b9742881f0",
   "metadata": {},
   "source": [
    "## Add Documents to the Vector Store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22599776-f7c7-47fc-84f9-8946f8da9d77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/schema/Question2 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/schema/Question2 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://10.79.253.112:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/schema \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7f68a86e91d0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "vector = WeaviateVectorStore.from_documents(docs, OllamaEmbeddings(model = \"nomic-embed-text:latest\", base_url=\"http://10.79.253.112:11434\"), client=client, index_name=\"Question2\", text_key=\"Question2\".lower() + \"_key\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eedbd2-d552-473f-b4b3-69adb44f38c2",
   "metadata": {},
   "source": [
    "## Retriever Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a73394d2-b53b-414c-b0dc-a0d86327a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vector.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b221ae9-07f6-462c-a916-e83d01b511a8",
   "metadata": {},
   "source": [
    "## Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11261d42-9027-41de-bde0-1519f336a923",
   "metadata": {},
   "source": [
    "`model=\"nvidia/nv-rerankqa-mistral-4b-v3\"`\n",
    "\n",
    "Specifies the name of the reranking model.\n",
    "This model is designed to re-order retrieved documents based on how well they answer a query (RAG-based QA).\n",
    "It is based on a 4B parameter Mistral architecture fine-tuned for reranking.\n",
    "\n",
    "`base_url=\"https://reranker-5c3f14b5-predictor-ezai-services.pcai1.genai1.hou\"`\n",
    "The endpoint URL of the hosted PCAI Reranker.\n",
    "\n",
    "`api_key=token`\n",
    "The API key used for authentication, stored in the variable token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fa2d767-9425-461c-9b79-27af7ad4bc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:154: UserWarning: https://reranker-5c3f14b5-predictor-ezai-services.pcai1.genai1.hou does not end in /v1, you may have inference and listing issues. This check will be deprecated in the next release. Please ensure /v1 is appended to the provided URL\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "compressor = NVIDIARerank(model=\"nvidia/nv-rerankqa-mistral-4b-v3\",\n",
    "                          base_url=\"https://reranker-5c3f14b5-predictor-ezai-services.pcai1.genai1.hou\",\n",
    "                          api_key=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1476c16-266c-4f73-b4e5-ee5c168d2338",
   "metadata": {},
   "source": [
    "## This compression_retriever performs a two-stage retrieval process:\n",
    "\n",
    "**1. Retrieve:** Fetch many potentially relevant chunks from a vector store.\n",
    "\n",
    "**2. Compress:** Rerank and filter those chunks using a more precise model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f66049a-8cba-4b4a-a09d-cdbdf7e120e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1786bc9-a8a3-49c9-b57a-effdd202942b",
   "metadata": {},
   "source": [
    "## User Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3399a7ae-b543-4d1d-8601-cd62fa4ca7bd",
   "metadata": {},
   "source": [
    "The user Question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a345a03-46b7-4c5d-8dba-86d8ae0f0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"which GPU powers the HPE ProLiant Compute DL384 Gen12 ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7c476-1e68-4797-a428-62a3893e1c4d",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff27b1-ed26-4d0c-a715-8e4660808baa",
   "metadata": {},
   "source": [
    "Invoke the chain with `chain.invoke`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04d6a4fc-832c-4e02-91aa-a174d8212e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://10.79.253.112:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'which GPU powers the HPE ProLiant Compute DL384 Gen12 ?',\n",
       " 'result': 'According to the provided QuickSpecs, the HPE ProLiant Compute DL384 Gen12 is enabled with the NVIDIA GH200 NVL2.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d622f3-6e4f-4f48-b829-ece014401453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
